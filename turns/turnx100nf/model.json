{
    "type": "non-fiction",
    "models": {
        "1.0": "maskito_v0.1.0_contextSpss10_large",
        "2.0": "maskito_v0.2.1_contextSpss10_large",
        "3.0": "maskito_v0.2.1_contextSpss32_mini"
    },
    "research_objectives": "<ol> <li>Compare different scoring parameters (list value 1 of this turn vs. list value from turn96) for the same model (maskito_v0.1.0_contextSpss10_large)</li><li>Compare performance of new 0.2.1 models (maskito_v0.2.1_contextSpss10_large vs maskito_v0.2.1_contextSpss32_mini)</li><li>Compare performance of maskito_v0.2.1_contextSpss32_mini with maskito_v0.2.0_contextSpss10_mini</li></ol>",
    "research_findings": "<ol> <li><u>On the comparison between the scoring parameters sim_df and is_paraphrase with the same model (maskito_v0.1.0_contextSpss10_large)</u> <br/>This comparison concerns the first list value from this turn and the first list value of turn 96. Both list values correspond to the model maskito_v0.1.0_contextSpss10_large with different scoring parameters, sim_dif for the current turn and is_paraphrase for turn 96. The mean summary table below shows that in most criteria, the same model with is_paraphrase scoring performed better than its counterpart with sim_dif. However, of these criteria, only identical_information shows a significant difference for the aforementioned scoring parameter. In contrast, the texts filtered with the sim_dif scoring were scored better in action_coherence, citation_correct, linguistic_difference and overall_quality. Of these criteria, only linguistic_difference prove to be significant. Furthermore, the model with sim_dif scoring produced more texts which were rated consistently higher than the same model with is_paraphrase (see violin plots for linguistic difference).<br/><b>In summary, the same model with sim_dif scoring produced more texts which were perceived as linguistically different without any significant change in overall_quality. However, the content of the paraphrases with sim_dif scoring was rated to be less identical with respect to the source text than with the is_paraphrase scoring.</b></li><br/> <li><u>On the comparison between maskito_v0.2.1_contextSpss10_large and maskito_v0.2.1_contextSpss32_mini</u> <br/>Consistent with previous turns, the large model outperforms the mini model in almost all criteria. In criteria where the mini model gave better results, i.e. exhaustive_information and identical_information, the difference is not significant.<br/><b>Overall, the large model produced texts which have better overall quality and less grammatical errors.</b> </li><br/> <li><u>On the comparison between maskito_v0.2.1_contextSpss32_mini and maskito_v0.2.0_contextSpss10_mini</u> <br/> The newer Maskito mini model (2.1, spss32?) scored better than the older model evaluated in turnx98. The only significant difference of these differences is readability but this value is to be taken with a grain of salt: the interrater agreement for this criterion in this turn across these models and across turns has always been low (0.367 in this turn).<br/><b>The mini model produced more grammatically correct texts which were also regarded by its readers as \"better\" paraphrases.</b></li><br/> <li><u>On the comparison between maskito_v0.1.4_contextSpss10_large and maskito_v0.2.1_contextSpss10_large</u> <br/> In general, the older SOTA model outperformed the newer 2.1 Maskito large model. This better performance can be seen in almost all rating criteria, save for linguistic difference. These differences, however, were not found to be significant. A slight dip in overall_quality could be observed with the newer larger model. Moreover, the violin plots show that the older large model produced paraphrases which were rated, on average, to be more similar in content to the corresponding input texts than the newer 0.2.1 model. The same trend could also be observed for the criterion grammar and identical_information.<br/><b>Although there were less fallback sentences used, Maskito_v0.2.1_contextSpss10_large produced paraphrases which were found to contain more grammatical errors and were less similar in content to the corresponding input texts in comparison to the older state of the art model. These differences, however, were not found to be significant.</b></li><br/></ol>"
}